{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')\n",
    "\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from utils.Vocabulary import Vocabulary\n",
    "from utils.ImageDataloader import get_image_dataloader, ImageDataset\n",
    "from models.ImageCaptioner import ImageCaptioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = os.environ['HOME'] + '/Database/coco/images/'\n",
    "captions_path = os.environ['HOME'] + '/programs/cocoapi/annotations/coco_captions.csv'\n",
    "models_path = '../models/'\n",
    "batch_size = 64\n",
    "coco_set = 2014\n",
    "load_features = True\n",
    "preload = True\n",
    "base_model='vgg16' # 'vgg16' # 'resnet152'\n",
    "embedding_size = 25088 # 25088 # 2048\n",
    "load_captions = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = '../data/processed/coco_vocab.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validation data...Done\n"
     ]
    }
   ],
   "source": [
    "print (\"Loading validation data...\\r\", end=\"\")\n",
    "val_loader = get_image_dataloader('val',coco_set,\n",
    "                                  images_path, \n",
    "                                  vocab_path, captions_path, \n",
    "                                  batch_size, \n",
    "                                  embedding_size=embedding_size,\n",
    "                                  load_features=load_features,\n",
    "                                  load_captions=load_captions,\n",
    "                                  model=base_model,\n",
    "                                  preload=preload)\n",
    "val_loader.dataset.mode = 'val'\n",
    "print (\"Loading validation data...Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = val_loader.dataset.get_vocab_size()\n",
    "start_id = val_loader.dataset.get_idx()[val_loader.dataset.vocab.start_word]\n",
    "end_id = val_loader.dataset.get_idx()[val_loader.dataset.vocab.end_word]\n",
    "max_caption_length = val_loader.dataset.max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "rnn_type = 'lstm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "captioner = ImageCaptioner(embedding_size, embed_size, \n",
    "                           hidden_size, vocab_size, \n",
    "                           max_caption_length, \n",
    "                           start_id, end_id)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  captioner.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../models/'\n",
    "model_path += 'image_caption-model10-80-0.1179-5.0.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageCaptioner(\n",
       "  (inp): Linear(in_features=25088, out_features=256, bias=True)\n",
       "  (inp_dropout): Dropout(p=0.2)\n",
       "  (inp_bn): BatchNorm1d(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "  (embed): Embedding(12433, 256)\n",
       "  (rnn): LSTM(256, 512, batch_first=True)\n",
       "  (out): Linear(in_features=512, out_features=12433, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(model_path)\n",
    "\n",
    "captioner.load_state_dict(checkpoint['params'])\n",
    "captioner.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation step [0/608], Bleu: 0.0943\n",
      "Validation step [250/608], Bleu: 0.1091\n",
      "Validation step [500/608], Bleu: 0.1104\n",
      "Validation step [607/608], Bleu: 0.1444\n",
      "Validation -- bleu: 0.1076\n"
     ]
    }
   ],
   "source": [
    "val_bleu = 0.0\n",
    "beam_size = 0\n",
    "\n",
    "for val_id, val_batch in enumerate(val_loader):\n",
    "  idxs, im_embeddings, caption_embeddings = val_batch\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "    im_embeddings = im_embeddings.cuda()\n",
    "    caption_embeddings = caption_embeddings.cuda()\n",
    "\n",
    "  # Get ground truth captions\n",
    "  refs = val_loader.dataset.get_references(idxs.numpy())\n",
    "            \n",
    "  preds = captioner.predict(im_embeddings, beam_size=beam_size)\n",
    "  \n",
    "  # Calculate bleu loss per sample in batch\n",
    "  # Sum and add length normalized sum to val_loss\n",
    "  batch_bleu = 0.0\n",
    "  for pred_id in range(len(preds)):\n",
    "    pred = preds[pred_id].cpu().numpy().astype(int)\n",
    "    pred_embed = val_loader.dataset.vocab.decode(pred, clean=True)\n",
    "    batch_bleu += val_loader.dataset.vocab.evaluate(refs[pred_id], pred_embed)\n",
    "  val_bleu += (batch_bleu/len(preds))\n",
    "\n",
    "  # Get training statistics\n",
    "  stats = \"Validation step [%d/%d], Bleu: %.4f\" \\\n",
    "            % (val_id, val_loader.dataset.get_seq_len(), \n",
    "                batch_bleu/len(preds))\n",
    "\n",
    "  print(\"\\r\" + stats, end=\"\")\n",
    "  sys.stdout.flush()\n",
    "\n",
    "  if val_id % 250 == 0:\n",
    "    print('\\r' + stats)\n",
    "\n",
    "val_bleu /= val_loader.dataset.get_seq_len()\n",
    "print (\"\\nValidation -- bleu: %.4f\" % (val_bleu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation step [0/608], Bleu: 0.0876\n",
      "Validation step [250/608], Bleu: 0.1547\n",
      "Validation step [500/608], Bleu: 0.1245\n",
      "Validation step [582/608], Bleu: 0.0905"
     ]
    }
   ],
   "source": [
    "val_bleu = 0.0\n",
    "beam_size = 3\n",
    "\n",
    "for val_id, val_batch in enumerate(val_loader):\n",
    "  idxs, im_embeddings, caption_embeddings = val_batch\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "    im_embeddings = im_embeddings.cuda()\n",
    "    caption_embeddings = caption_embeddings.cuda()\n",
    "\n",
    "  # Get ground truth captions\n",
    "  refs = val_loader.dataset.get_references(idxs.numpy())\n",
    "            \n",
    "  preds = captioner.predict(im_embeddings, beam_size=beam_size)\n",
    "  \n",
    "  # Calculate bleu loss per sample in batch\n",
    "  # Sum and add length normalized sum to val_loss\n",
    "  batch_bleu = 0.0\n",
    "  for pred_id in range(len(preds)):\n",
    "    pred = preds[pred_id].cpu().numpy().astype(int)\n",
    "    pred_embed = val_loader.dataset.vocab.decode(pred, clean=True)\n",
    "    batch_bleu += val_loader.dataset.vocab.evaluate(refs[pred_id], pred_embed)\n",
    "  val_bleu += (batch_bleu/len(preds))\n",
    "\n",
    "  # Get training statistics\n",
    "  stats = \"Validation step [%d/%d], Bleu: %.4f\" \\\n",
    "            % (val_id, val_loader.dataset.get_seq_len(), \n",
    "                batch_bleu/len(preds))\n",
    "\n",
    "  print(\"\\r\" + stats, end=\"\")\n",
    "  sys.stdout.flush()\n",
    "\n",
    "  if val_id % 250 == 0:\n",
    "    print('\\r' + stats)\n",
    "\n",
    "val_bleu /= val_loader.dataset.get_seq_len()\n",
    "print (\"\\nValidation -- bleu: %.4f\" % (val_bleu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-575369ecd497>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mrefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_references\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaptioner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeam_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;31m# Calculate bleu loss per sample in batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/narrator/src/models/ImageCaptioner.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, im_embeddings, return_probs, beam_size, desired_num_captions)\u001b[0m\n\u001b[1;32m    201\u001b[0m                         \u001b[0mreturn_probs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_probs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                         \u001b[0mbeam_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeam_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m                         top_k=desired_num_captions)\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0;31m# Remove extra dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/narrator/src/models/ImageCaptioner.py\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(self, output, hidden, return_probs, beam_size, top_k)\u001b[0m\n\u001b[1;32m    257\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeam_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                     \u001b[0mnext_idx_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m                     \u001b[0mnext_idx_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m                     \u001b[0mlog_prob\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtop_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "val_bleu = 0.0\n",
    "beam_size = 5\n",
    "\n",
    "for val_id, val_batch in enumerate(val_loader):\n",
    "  idxs, im_embeddings, caption_embeddings = val_batch\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "    im_embeddings = im_embeddings.cuda()\n",
    "    caption_embeddings = caption_embeddings.cuda()\n",
    "\n",
    "  # Get ground truth captions\n",
    "  refs = val_loader.dataset.get_references(idxs.numpy())\n",
    "            \n",
    "  preds = captioner.predict(im_embeddings, beam_size=beam_size)\n",
    "  \n",
    "  # Calculate bleu loss per sample in batch\n",
    "  # Sum and add length normalized sum to val_loss\n",
    "  batch_bleu = 0.0\n",
    "  for pred_id in range(len(preds)):\n",
    "    pred = preds[pred_id].cpu().numpy().astype(int)\n",
    "    pred_embed = val_loader.dataset.vocab.decode(pred, clean=True)\n",
    "    batch_bleu += val_loader.dataset.vocab.evaluate(refs[pred_id], pred_embed)\n",
    "  val_bleu += (batch_bleu/len(preds))\n",
    "\n",
    "  # Get training statistics\n",
    "  stats = \"Validation step [%d/%d], Bleu: %.4f\" \\\n",
    "            % (val_id, val_loader.dataset.get_seq_len(), \n",
    "                batch_bleu/len(preds))\n",
    "\n",
    "  print(\"\\r\" + stats, end=\"\")\n",
    "  sys.stdout.flush()\n",
    "\n",
    "  if val_id % 250 == 0:\n",
    "    print('\\r' + stats)\n",
    "\n",
    "val_bleu /= val_loader.dataset.get_seq_len()\n",
    "print (\"\\nValidation -- bleu: %.4f\" % (val_bleu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
