{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')\n",
    "\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from utils.Vocabulary import Vocabulary\n",
    "from utils.ImageDataloader import get_image_dataloader, ImageDataset\n",
    "from models.ImageCaptioner import ImageCaptioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = os.environ['HOME'] + '/Database/coco/images/'\n",
    "captions_path = os.environ['HOME'] + '/programs/cocoapi/annotations/coco_captions.csv'\n",
    "models_path = '../models/'\n",
    "batch_size = 64\n",
    "coco_set = 2014\n",
    "load_features = True\n",
    "preload = True\n",
    "base_model='resnet152' # 'vgg16' # 'resnet152'\n",
    "embedding_size = 2048 # 25088 # 2048\n",
    "load_captions = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = '../data/processed/coco_vocab.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validation data...Done\n"
     ]
    }
   ],
   "source": [
    "print (\"Loading validation data...\\r\", end=\"\")\n",
    "val_loader = get_image_dataloader('val',coco_set,\n",
    "                                  images_path, \n",
    "                                  vocab_path, captions_path, \n",
    "                                  batch_size, \n",
    "                                  embedding_size=embedding_size,\n",
    "                                  load_features=load_features,\n",
    "                                  load_captions=load_captions,\n",
    "                                  model=base_model,\n",
    "                                  preload=preload)\n",
    "val_loader.dataset.mode = 'val'\n",
    "print (\"Loading validation data...Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = val_loader.dataset.get_vocab_size()\n",
    "start_id = val_loader.dataset.get_idx()[val_loader.dataset.vocab.start_word]\n",
    "end_id = val_loader.dataset.get_idx()[val_loader.dataset.vocab.end_word]\n",
    "max_caption_length = val_loader.dataset.max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "rnn_type = 'lstm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "captioner = ImageCaptioner(embedding_size, embed_size, \n",
    "                           hidden_size, vocab_size, \n",
    "                           max_caption_length, \n",
    "                           start_id, end_id)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  captioner.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../models/'\n",
    "model_path += 'image_caption-model4-20-0.1328-5.0.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageCaptioner(\n",
       "  (inp): Linear(in_features=2048, out_features=256, bias=True)\n",
       "  (inp_dropout): Dropout(p=0.2)\n",
       "  (inp_bn): BatchNorm1d(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "  (embed): Embedding(12433, 256)\n",
       "  (rnn): LSTM(256, 512, batch_first=True)\n",
       "  (out): Linear(in_features=512, out_features=12433, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(model_path)\n",
    "\n",
    "captioner.load_state_dict(checkpoint['params'])\n",
    "captioner.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation step [0/608], Bleu: 0.1239\n",
      "Validation step [250/608], Bleu: 0.1104\n",
      "Validation step [500/608], Bleu: 0.1356\n",
      "Validation step [607/608], Bleu: 0.1297\n",
      "Validation -- bleu: 0.1230\n"
     ]
    }
   ],
   "source": [
    "val_bleu = 0.0\n",
    "beam_size = 0\n",
    "\n",
    "for val_id, val_batch in enumerate(val_loader):\n",
    "  idxs, im_embeddings, caption_embeddings = val_batch\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "    im_embeddings = im_embeddings.cuda()\n",
    "    caption_embeddings = caption_embeddings.cuda()\n",
    "\n",
    "  # Get ground truth captions\n",
    "  refs = val_loader.dataset.get_references(idxs.numpy())\n",
    "            \n",
    "  preds = captioner.predict(im_embeddings, beam_size=beam_size)\n",
    "  \n",
    "  # Calculate bleu loss per sample in batch\n",
    "  # Sum and add length normalized sum to val_loss\n",
    "  batch_bleu = 0.0\n",
    "  for pred_id in range(len(preds)):\n",
    "    pred = preds[pred_id].cpu().numpy().astype(int)\n",
    "    pred_embed = val_loader.dataset.vocab.decode(pred, clean=True)\n",
    "    batch_bleu += val_loader.dataset.vocab.evaluate(refs[pred_id], pred_embed)\n",
    "  val_bleu += (batch_bleu/len(preds))\n",
    "\n",
    "  # Get training statistics\n",
    "  stats = \"Validation step [%d/%d], Bleu: %.4f\" \\\n",
    "            % (val_id, val_loader.dataset.get_seq_len(), \n",
    "                batch_bleu/len(preds))\n",
    "\n",
    "  print(\"\\r\" + stats, end=\"\")\n",
    "  sys.stdout.flush()\n",
    "\n",
    "  if val_id % 250 == 0:\n",
    "    print('\\r' + stats)\n",
    "\n",
    "val_bleu /= val_loader.dataset.get_seq_len()\n",
    "print (\"\\nValidation -- bleu: %.4f\" % (val_bleu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation step [0/608], Bleu: 0.1114\n",
      "Validation step [250/608], Bleu: 0.1684\n",
      "Validation step [500/608], Bleu: 0.0994\n",
      "Validation step [607/608], Bleu: 0.1171\n",
      "Validation -- bleu: 0.1323\n"
     ]
    }
   ],
   "source": [
    "val_bleu = 0.0\n",
    "beam_size = 3\n",
    "\n",
    "for val_id, val_batch in enumerate(val_loader):\n",
    "  idxs, im_embeddings, caption_embeddings = val_batch\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "    im_embeddings = im_embeddings.cuda()\n",
    "    caption_embeddings = caption_embeddings.cuda()\n",
    "\n",
    "  # Get ground truth captions\n",
    "  refs = val_loader.dataset.get_references(idxs.numpy())\n",
    "            \n",
    "  preds = captioner.predict(im_embeddings, beam_size=beam_size)\n",
    "  \n",
    "  # Calculate bleu loss per sample in batch\n",
    "  # Sum and add length normalized sum to val_loss\n",
    "  batch_bleu = 0.0\n",
    "  for pred_id in range(len(preds)):\n",
    "    pred = preds[pred_id].cpu().numpy().astype(int)\n",
    "    pred_embed = val_loader.dataset.vocab.decode(pred, clean=True)\n",
    "    batch_bleu += val_loader.dataset.vocab.evaluate(refs[pred_id], pred_embed)\n",
    "  val_bleu += (batch_bleu/len(preds))\n",
    "\n",
    "  # Get training statistics\n",
    "  stats = \"Validation step [%d/%d], Bleu: %.4f\" \\\n",
    "            % (val_id, val_loader.dataset.get_seq_len(), \n",
    "                batch_bleu/len(preds))\n",
    "\n",
    "  print(\"\\r\" + stats, end=\"\")\n",
    "  sys.stdout.flush()\n",
    "\n",
    "  if val_id % 250 == 0:\n",
    "    print('\\r' + stats)\n",
    "\n",
    "val_bleu /= val_loader.dataset.get_seq_len()\n",
    "print (\"\\nValidation -- bleu: %.4f\" % (val_bleu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation step [0/608], Bleu: 0.1505\n",
      "Validation step [250/608], Bleu: 0.1429\n",
      "Validation step [500/608], Bleu: 0.1335\n",
      "Validation step [527/608], Bleu: 0.1282"
     ]
    }
   ],
   "source": [
    "val_bleu = 0.0\n",
    "beam_size = 5\n",
    "\n",
    "for val_id, val_batch in enumerate(val_loader):\n",
    "  idxs, im_embeddings, caption_embeddings = val_batch\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "    im_embeddings = im_embeddings.cuda()\n",
    "    caption_embeddings = caption_embeddings.cuda()\n",
    "\n",
    "  # Get ground truth captions\n",
    "  refs = val_loader.dataset.get_references(idxs.numpy())\n",
    "            \n",
    "  preds = captioner.predict(im_embeddings, beam_size=beam_size)\n",
    "  \n",
    "  # Calculate bleu loss per sample in batch\n",
    "  # Sum and add length normalized sum to val_loss\n",
    "  batch_bleu = 0.0\n",
    "  for pred_id in range(len(preds)):\n",
    "    pred = preds[pred_id].cpu().numpy().astype(int)\n",
    "    pred_embed = val_loader.dataset.vocab.decode(pred, clean=True)\n",
    "    batch_bleu += val_loader.dataset.vocab.evaluate(refs[pred_id], pred_embed)\n",
    "  val_bleu += (batch_bleu/len(preds))\n",
    "\n",
    "  # Get training statistics\n",
    "  stats = \"Validation step [%d/%d], Bleu: %.4f\" \\\n",
    "            % (val_id, val_loader.dataset.get_seq_len(), \n",
    "                batch_bleu/len(preds))\n",
    "\n",
    "  print(\"\\r\" + stats, end=\"\")\n",
    "  sys.stdout.flush()\n",
    "\n",
    "  if val_id % 250 == 0:\n",
    "    print('\\r' + stats)\n",
    "\n",
    "val_bleu /= val_loader.dataset.get_seq_len()\n",
    "print (\"\\nValidation -- bleu: %.4f\" % (val_bleu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
